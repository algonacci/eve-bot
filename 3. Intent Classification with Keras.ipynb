{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Classification with Keras\n",
    "In my past notebooks, my goal was to recieve my labeled data for my chatbot. Now this notebook focuses on using Keras to classify intents of new, unseen data that a user might type up. The model now switched to a supervised learning approach now that we generated the labels from the unsupervised learning we did in the previous notebook.\n",
    "\n",
    "### Rasa Comparison\n",
    "Rasa trains this intent classification step with SVM and GridsearchCV because they can try different configurations ([source](https://medium.com/bhavaniravi/intent-classification-demystifying-rasanlu-part-4-685fc02f5c1d)). When deploying preprocessing pipeline should remain same between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pandas: 1.0.5\nNumpy: 1.18.5\nTensorflow: 2.2.0\nKeras: 2.3.0-tf\nSklearn: 0.23.1\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ae4297c74d84055ac9cc514e0706e44"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n                                           Utterance   Intent\n0  [hey, please, fix, io, battery, drain, issue, ...  Battery\n1  [okay, new, update, io, still, see, question, ...   Update\n2       [iphone, slow, dial, since, new, io, update]   iphone\n3  [also, anytime, wan, na, fix, i️, thing, annny...      app\n4  [hi, instal, high, update, day, ago, start, mo...      mac\n\nintents:\n{'app': ['app', 'application'], 'battery': ['battery'], 'bug': ['bug'], 'greeting': ['hi', 'hello', 'hey', 'yo'], 'icloud': ['icloud', 'i cloud'], 'ios': ['io'], 'iphone': ['iphone', 'i phone'], 'mac': ['mac', 'macbook', 'laptop', 'computer'], 'music': ['music', 'song', 'playlist'], 'payment': ['credit', 'card', 'payment', 'pay'], 'settings': ['settings', 'setting'], 'troubleshooting': ['problem', 'trouble'], 'update': ['update'], 'watch': ['watch']}\n\nrepresentative intents:\n{'Battery': ['io', 'drain', 'battery', 'iphone', 'twice', 'fast', 'io', 'help'], 'Update': ['new', 'update', 'i️', 'make', 'sure', 'download', 'yesterday'], 'app': ['app', 'still', 'longer', 'able', 'control', 'lockscreen'], 'greeting': ['hi', 'hello', 'yo', 'hey', 'whats', 'up'], 'iphone': ['instal', 'io', 'make', 'iphone', 'slow', 'work', 'properly', 'help'], 'mac': ['help', 'mac', 'app', 'store', 'open', 'can', 'not', 'update', 'macbook', 'pro', 'currently', 'run', 'o', 'x', 'yosemite']}\n"
    }
   ],
   "source": [
    "# Data science\n",
    "import pandas as pd\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "# Deep Learning \n",
    "import tensorflow as tf\n",
    "print(f\"Tensorflow: {tf.__version__}\")\n",
    "from tensorflow import keras\n",
    "print(f\"Keras: {keras.__version__}\")\n",
    "import sklearn\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Cool progress bars\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()  # Enable tracking of execution progress\n",
    "\n",
    "import collections\n",
    "import yaml\n",
    "\n",
    "# Preprocessing and Keras\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reading back in intents\n",
    "with open(r'objects/intents.yml') as file:\n",
    "    intents = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Reading in representative intents\n",
    "with open(r'objects/intents_repr.yml') as file:\n",
    "    intents_repr = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "# Reading in training data\n",
    "train = pd.read_pickle('objects/train.pkl')\n",
    "\n",
    "print(train.head())\n",
    "print(f'\\nintents:\\n{intents}')\n",
    "print(f'\\nrepresentative intents:\\n{intents_repr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_bagofwords(data, output_name, title):\n",
    "    ''' Taking as input the data and plots the top 10 words based on counts in this text data'''\n",
    "    bagofwords = CountVectorizer()\n",
    "    inbound = bagofwords.fit_transform(data)\n",
    "    \n",
    "    # Make rank\n",
    "    word_counts = np.array(np.sum(inbound, axis=0)).reshape((-1,))\n",
    "    words = np.array(bagofwords.get_feature_names())\n",
    "    words_df = pd.DataFrame({\"word\":words, \n",
    "                             \"count\":word_counts})\n",
    "    words_rank = words_df.sort_values(by=\"count\", ascending=False)\n",
    "    \n",
    "    # Visualizing top 10 words\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(words_rank['word'][:10], words_rank['count'][:10].astype(str), palette = 'inferno')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Saving\n",
    "    plt.savefig(f'visualizations/{output_name}.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Preprocessing\n",
    "I've done most of the main preprocessing work, but Keras needs some more specific things for to model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Vocab Size: 4164 \nMax Token Length: 32\nShape of padded tweets: (5000, 32)\n\nPreview of encoded and padded tweets:\n [[ 38  11   6 ...   0   0   0]\n [368   7   1 ...   0   0   0]\n [  3  22 870 ...   0   0   0]\n ...\n [  1   2   4 ...   0   0   0]\n [  2   1 125 ...   0   0   0]\n [  8  26   2 ...   0   0   0]]\n\nPreview of intent representation:\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n ...\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]]\n\nShape checks:\nX_train: (3500, 32) X_val: (1500, 32)\ny_train: (3500, 5) y_val: (1500, 5)\n"
    }
   ],
   "source": [
    "# 1. Create tokenizer object\n",
    "def make_tokenizer(docs, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    t = Tokenizer(filters = filters)\n",
    "    t.fit_on_texts(docs)\n",
    "    return t\n",
    "\n",
    "token = make_tokenizer(train['Utterance'])\n",
    "\n",
    "# 2. Finding length of vocabulary\n",
    "vocab_size = len(token.word_index) + 1\n",
    "\n",
    "# 3. Finding maximum length of Tokens\n",
    "get_max_token_length = lambda series: len(max(series, key = len))\n",
    "max_token_length = get_max_token_length(train['Utterance'])\n",
    "\n",
    "print(f'Vocab Size: {vocab_size} \\nMax Token Length: {max_token_length}')\n",
    "\n",
    "# 4. Encode documents - matching with Keras dictionary\n",
    "encode_tweets = lambda token, words: token.texts_to_sequences(words)\n",
    "encoded_tweets = encode_tweets(token, train['Utterance'])\n",
    "\n",
    "# 5. Padding my documents - filling with tags to normalize the lengths\n",
    "pad_tweets = lambda encoded_doc, max_length: pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\")\n",
    "\n",
    "padded_tweets = pad_tweets(encoded_tweets, max_token_length)\n",
    "print(\"Shape of padded tweets:\",padded_tweets.shape)\n",
    "print(\"\\nPreview of encoded and padded tweets:\\n\", padded_tweets)\n",
    "\n",
    "# 6. One hot encode to represent target variable data (intents)\n",
    "one_hot = lambda encode: OneHotEncoder(sparse = False).fit_transform(encode)\n",
    "unique_intents = list(set(train['Intent']))\n",
    "# making another tokenizer\n",
    "output_tokenizer = make_tokenizer(unique_intents, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n",
    "encoded_intents = encode_tweets(output_tokenizer, train['Intent'])\n",
    "# reshaping encoded Tweets for this one hot function\n",
    "encoded_intents = np.array(encoded_intents).reshape(len(encoded_intents), 1)\n",
    "one_hot_intents = one_hot(encoded_intents)\n",
    "print(f'\\nPreview of intent representation:\\n{one_hot_intents}')\n",
    "\n",
    "# 7. Split in to train and test\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_tweets, one_hot_intents, test_size = 0.3, \n",
    "                                                   shuffle = True, stratify = one_hot_intents)\n",
    "print(f'\\nShape checks:\\nX_train: {X_train.shape} X_val: {X_val.shape}\\ny_train: {y_train.shape} y_val: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models look for y variables to be one hot encoded. When it's multiclass many people feed it as one hot encoded vectors. It's just one of the design choices.\n",
    "\n",
    "If you're using doc2vec embeddings, how do you pass in your Tweets. You may have to pass it in as full tweets. Check how you pass in the Tweets. You may have to tokenize at a Tweet level. If you pass it in, if it's Tweet 57, it will activate the node such that it gets multiplied out by the embeddings for the 57th document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making my own embedding matrix that's in a specific order\n",
    "d2v_embedding_matrix = pd.read_pickle('objects/inbound_d2v.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a regular word embedding, the order of the embeddings in the matrix has to be setup so that it matches how the words appear in my keras tokenizer word index. It does it so that the most common words appear up front, and the embedding matrix needs to be aligned.\n",
    "\n",
    "Make sure the order of the embeddings is going to be the order that the embeddings are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Modelling\n",
    "I will create a neural network with Keras with the output layer having the same number of nodes as there are intents. The following is my architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(vocab_size, max_token_length):\n",
    "    ''' In this function I define all the layers of my neural network'''\n",
    "    # Initialize\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adding layers\n",
    "    model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "#    model.add(LSTM(128)) # Another LSTM layer if things aren't doing well. Beef up the size of the Dense layer\n",
    "    model.add(Dense(32, activation = \"relu\")) # Try 50, another dense layer? This takes a little bit of exploration\n",
    "    \n",
    "    # Only update 50 percent of the nodes\n",
    "    model.add(Dropout(0.5))\n",
    "    # Make sure when you update your number of unique intents, you reflect it in this last layer\n",
    "    model.add(Dense(22, activation = \"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "hist = model.fit(train_X, train_Y, epochs = 100, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    print(test_word)\n",
    "    # Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "    x = padding_doc(test_ls, max_length)\n",
    "    pred = model.predict_proba(x)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    predictions = pred[0]\n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    "    for i in range(pred.shape[1]):\n",
    "        print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"are you a robot\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Different Intent Classification Methods\n",
    "\n",
    "There already exists chatbot frameworks people use, such as Wix.\n",
    "\n",
    "Still, hopefully the intent classification of my Tweets would be good enough. The problem with my Tweets is that there are a lot of noise, and it's not clear what intent a particular Tweet represents. Also, I will definitely have class imbalance, in which I would have to do upsampling or down sampling to get my classification accuracies to be good.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "    ''' In this function I define all the layers of my neural network'''\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "#   model.add(LSTM(128)) # Another LSTM layer if things aren't doing well. Beef up the size of the Dense layer\n",
    "    model.add(Dense(32, activation = \"relu\")) # Try 50, another dense layer? This takes a little bit of exploration\n",
    "    \n",
    "    # Only update 50 percent of the nodes\n",
    "    model.add(Dropout(0.5))\n",
    "    # Make sure when you update your number of unique intents, you reflect it in this last layer\n",
    "    model.add(Dense(22, activation = \"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multilabel classification, you use sigmoid. You'll still have 10 distinct intents. But you need to modelled such that each of those intents are independent of each other.\n",
    "\n",
    "Prediction of intent 1 shouldn't effect intent 2. Softmax takes all the scores over all classes, and the highest number will have highest probability output but everything will sum to 1. For the final softmax layer will sum to 1, but that doesn't work in my case.\n",
    "\n",
    "But you're going to classify each intent separately. They can sum to greater than 1. \n",
    "\n",
    "Similar to logreg in multiclass. One curve for class 0 and not class 0. The sum of those probs can be greater than 1.\n",
    "\n",
    "You would use sigmoid for the activation function.\n",
    "\n",
    "For class 1, itll be 1 or not 1. Etc. You would look at your output layer, and whichever nodes have a greater probability output of >0.5, those 2 are your final output. You can do up to 3. Depends on how many nodes you will have.\n",
    "\n",
    "When you feed in your target vector, they need to go into the one hot encoded vectors. Target column will have 10 columns. It will all sum up to one for each node. Each node will have a separate sigmoid function (P (1-P)). Across the nodes they're going to sum more than 1. One versus rest classification. Read up based on logreg terms. Multilabel classification. The main thing is your labels need to one hot encode. Loss function would use binary cross entropy.\n",
    "\n",
    "Issues: the more classes you have, the harder it is for your model. Especially for the 2nd and 3rd label that's when accs start to drop a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question and Answering Example with Tensorflow\n",
    "Found at [Hugging Face](https://huggingface.co/transformers/task_summary.html#sequence-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face constantly updating with pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of question answering using a model and a tokenizer. The process is the following:\n",
    "\n",
    "Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
    "Define a text and a few questions.\n",
    "Iterate over the questions and build a sequence from the text and the current question, with the correct model-specific separators token type ids and attention masks\n",
    "Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    "Compute the softmax of the result to get probabilities over the tokens\n",
    "Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
    "Print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenizer\n",
    "\n",
    "Creates a dictionary of all the words in the vocab, and it stores the index. For each sequence it passes in the sequence and converts each word into the index that refers to the Keras word dictionary. When you feed in sentences into the model, they all have to be the same length. But some tweets are going to be longer than others, so pad_sequences just pad all the other ones so they are on the same length. It padding the messages with 0s until they are the same length as the longest message. They might set a max-length that are shorter because longer sequences are harder to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got this tokenizer function from https://www.tensorflow.org/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.preprocessing.text.Tokenizer(\n",
    "#     num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "#     split=' ', char_level=False, oov_token=None, document_count=0\n",
    "# )\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "tokenized = tokenize(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting my model\n",
    "When I build my neural network with K fold cross validation, it will take a LONG time so you can probably get away without doing CV and hyperparamater optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}